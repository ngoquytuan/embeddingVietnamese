# embedding_evaluator_windows.py - Phi√™n b·∫£n Windows-friendly
import json
import os
import re
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Any
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer, util
import torch

# Thi·∫øt l·∫≠p logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('embedding_evaluation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class VietnameseSentenceSplitter:
    """
    T√°ch c√¢u ti·∫øng Vi·ªát b·∫±ng regex - thay th·∫ø cho underthesea
    """
    def __init__(self):
        # Pattern ƒë·ªÉ t√°ch c√¢u ti·∫øng Vi·ªát
        self.sentence_endings = r'[.!?‚Ä¶]\s*'
        self.abbreviations = {
            'TP.', 'Q.', 'P.', 'St.', 'Dr.', 'Mr.', 'Mrs.', 'Ms.',
            'Prof.', 'vs.', 'etc.', 'Ltd.', 'Inc.', 'Co.',
            'Th.S', 'TS.', 'GS.', 'PGS.'
        }
    
    def split_sentences(self, text: str) -> List[str]:
        """T√°ch c√¢u b·∫±ng regex"""
        # X·ª≠ l√Ω tr∆∞·ªõc c√°c vi·∫øt t·∫Øt
        protected_text = text
        for abbr in self.abbreviations:
            protected_text = protected_text.replace(abbr, abbr.replace('.', '<!DOT!>'))
        
        # T√°ch c√¢u
        sentences = re.split(self.sentence_endings, protected_text)
        
        # Kh√¥i ph·ª•c d·∫•u ch·∫•m trong vi·∫øt t·∫Øt
        sentences = [s.replace('<!DOT!>', '.').strip() for s in sentences if s.strip()]
        
        return sentences

class SimpleVietnameseTokenizer:
    """
    Tokenizer ƒë∆°n gi·∫£n cho ti·∫øng Vi·ªát - thay th·∫ø cho PyVi
    """
    def tokenize(self, text: str) -> str:
        """Tokenize ƒë∆°n gi·∫£n b·∫±ng c√°ch t√°ch t·ª´"""
        # X·ª≠ l√Ω d·∫•u c√¢u
        text = re.sub(r'([.!?,:;])', r' \1 ', text)
        # T√°ch t·ª´
        words = text.split()
        return ' '.join(words)

class VietnameseEmbeddingEvaluator:
    def __init__(self, config_path: str = "config.json"):
        """
        Kh·ªüi t·∫°o evaluator v·ªõi file config
        """
        self.config = self.load_config(config_path)
        self.results = {}
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Kh·ªüi t·∫°o Vietnamese NLP tools
        self.sentence_splitter = VietnameseSentenceSplitter()
        self.tokenizer = SimpleVietnameseTokenizer()
        
        logger.info(f"S·ª≠ d·ª•ng device: {self.device}")
        logger.info(f"S·ª≠ d·ª•ng Vietnamese sentence splitter: {type(self.sentence_splitter).__name__}")
        
    def load_config(self, config_path: str) -> Dict:
        """Load c·∫•u h√¨nh t·ª´ file JSON"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"ƒê√£ load config t·ª´ {config_path}")
            return config
        except Exception as e:
            logger.error(f"L·ªói khi load config: {e}")
            raise
    
    def load_document_from_md(self, file_path: str) -> str:
        """Load vƒÉn b·∫£n t·ª´ file markdown"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            logger.info(f"ƒê√£ load document t·ª´ {file_path} ({len(content)} k√Ω t·ª±)")
            return content
        except Exception as e:
            logger.error(f"L·ªói khi load document t·ª´ {file_path}: {e}")
            raise
    
    def chunk_text(self, text: str, chunk_size: int = None) -> List[str]:
        """
        Chia t√†i li·ªáu th√†nh chunks v·ªõi t√°ch c√¢u ti·∫øng Vi·ªát
        """
        if chunk_size is None:
            chunk_size = self.config.get("evaluation_settings", {}).get("chunk_size", 200)
        
        logger.info(f"B·∫Øt ƒë·∫ßu chia text th√†nh chunks v·ªõi size={chunk_size}")
        
        try:
            # T√°ch c√¢u b·∫±ng regex splitter
            sentences = self.sentence_splitter.split_sentences(text)
            logger.info(f"ƒê√£ t√°ch ƒë∆∞·ª£c {len(sentences)} c√¢u")
            
            chunks = []
            current_chunk = ""
            current_word_count = 0
            
            for i, sentence in enumerate(sentences):
                # Tokenize t·ª´ b·∫±ng simple tokenizer
                words = self.tokenizer.tokenize(sentence).split()
                
                if current_word_count + len(words) <= chunk_size:
                    current_chunk += " " + sentence if current_chunk else sentence
                    current_word_count += len(words)
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                        logger.debug(f"Chunk {len(chunks)}: {len(current_chunk)} chars, {current_word_count} words")
                    
                    current_chunk = sentence
                    current_word_count = len(words)
            
            # Th√™m chunk cu·ªëi c√πng
            if current_chunk:
                chunks.append(current_chunk.strip())
                logger.debug(f"Chunk {len(chunks)}: {len(current_chunk)} chars, {current_word_count} words")
            
            logger.info(f"ƒê√£ chia th√†nh {len(chunks)} chunks")
            return chunks
            
        except Exception as e:
            logger.error(f"L·ªói khi chia text: {e}")
            raise
    
    def evaluate_model(self, model_name: str, chunks: List[str]) -> Dict[str, Any]:
        """
        ƒê√°nh gi√° m·ªôt embedding model v·ªõi c√°c chunks
        """
        logger.info(f"B·∫Øt ƒë·∫ßu ƒë√°nh gi√° model: {model_name}")
        
        try:
            # Load model
            logger.info(f"ƒêang load model {model_name}...")
            model = SentenceTransformer(model_name, device=self.device)
            
            # Th√¥ng tin model
            embedding_dim = model.get_sentence_embedding_dimension()
            logger.info(f"S·ªë chi·ªÅu embedding: {embedding_dim}")
            
            # T·∫°o embeddings
            logger.info(f"ƒêang t·∫°o embeddings cho {len(chunks)} chunks...")
            embeddings = model.encode(
                chunks, 
                convert_to_tensor=True,
                show_progress_bar=True,
                batch_size=32
            )
            
            logger.info(f"Shape c·ªßa embeddings: {embeddings.shape}")
            
            # T√≠nh similarity matrix
            similarity_matrix = util.cos_sim(embeddings, embeddings).cpu().numpy()
            
            # C√°c metrics ƒë√°nh gi√°
            metrics = self.calculate_metrics(similarity_matrix, chunks)
            
            result = {
                "model_name": model_name,
                "embedding_dimension": embedding_dim,
                "num_chunks": len(chunks),
                "device_used": str(model.device),
                "similarity_matrix": similarity_matrix,
                "metrics": metrics,
                "status": "success"
            }
            
            logger.info(f"ƒê√°nh gi√° th√†nh c√¥ng model {model_name}")
            return result
            
        except Exception as e:
            logger.error(f"L·ªói khi ƒë√°nh gi√° model {model_name}: {e}")
            return {
                "model_name": model_name,
                "status": "error",
                "error": str(e)
            }
    
    def calculate_metrics(self, similarity_matrix: np.ndarray, chunks: List[str]) -> Dict[str, float]:
        """
        T√≠nh c√°c metrics ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng embedding
        """
        logger.debug("ƒêang t√≠nh to√°n metrics...")
        
        # Lo·∫°i b·ªè ƒë∆∞·ªùng ch√©o ch√≠nh (similarity c·ªßa chunk v·ªõi ch√≠nh n√≥)
        n = similarity_matrix.shape[0]
        mask = ~np.eye(n, dtype=bool)
        similarities = similarity_matrix[mask]
        
        metrics = {
            "avg_similarity": float(np.mean(similarities)),
            "std_similarity": float(np.std(similarities)),
            "min_similarity": float(np.min(similarities)),
            "max_similarity": float(np.max(similarities)),
            "median_similarity": float(np.median(similarities)),
            "similarity_range": float(np.max(similarities) - np.min(similarities))
        }
        
        # T√≠nh ph·∫ßn trƒÉm similarities tr√™n threshold
        threshold = self.config.get("evaluation_settings", {}).get("similarity_threshold", 0.7)
        high_sim_count = np.sum(similarities > threshold)
        metrics["high_similarity_ratio"] = float(high_sim_count / len(similarities))
        
        logger.debug(f"Metrics calculated: {metrics}")
        return metrics
    
    def evaluate_all_models(self, chunks: List[str]) -> Dict[str, Any]:
        """
        ƒê√°nh gi√° t·∫•t c·∫£ models trong config
        """
        logger.info("B·∫Øt ƒë·∫ßu ƒë√°nh gi√° t·∫•t c·∫£ models...")
        
        results = {}
        models = self.config.get("models", [])
        
        for i, model_config in enumerate(models, 1):
            model_name = model_config["name"]
            logger.info(f"ƒê√°nh gi√° model {i}/{len(models)}: {model_name}")
            
            result = self.evaluate_model(model_name, chunks)
            result["model_description"] = model_config.get("description", "")
            result["language_support"] = model_config.get("language_support", "")
            
            results[model_name] = result
        
        return results
    
    def generate_report(self, results: Dict[str, Any], output_dir: str = "reports") -> str:
        """
        T·∫°o b√°o c√°o so s√°nh c√°c models
        """
        logger.info("ƒêang t·∫°o b√°o c√°o...")
        
        # T·∫°o th∆∞ m·ª•c output
        Path(output_dir).mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"{output_dir}/embedding_evaluation_report_{timestamp}.html"
        
        # Chu·∫©n b·ªã data cho b√°o c√°o
        successful_results = {k: v for k, v in results.items() if v.get("status") == "success"}
        
        if not successful_results:
            logger.error("Kh√¥ng c√≥ model n√†o ƒë√°nh gi√° th√†nh c√¥ng!")
            return None
        
        # T·∫°o DataFrame cho comparison
        comparison_data = []
        for model_name, result in successful_results.items():
            metrics = result.get("metrics", {})
            row = {
                "Model": model_name,
                "Description": result.get("model_description", ""),
                "Language Support": result.get("language_support", ""),
                "Embedding Dimension": result.get("embedding_dimension", "N/A"),
                "Num Chunks": result.get("num_chunks", "N/A"),
                "Avg Similarity": f"{metrics.get('avg_similarity', 0):.4f}",
                "Std Similarity": f"{metrics.get('std_similarity', 0):.4f}",
                "Similarity Range": f"{metrics.get('similarity_range', 0):.4f}",
                "High Sim Ratio": f"{metrics.get('high_similarity_ratio', 0):.2%}",
                "Status": result.get("status", "unknown")
            }
            comparison_data.append(row)
        
        df = pd.DataFrame(comparison_data)
        
        # T·∫°o HTML report (t∆∞∆°ng t·ª± nh∆∞ tr∆∞·ªõc)
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Vietnamese Embedding Models Evaluation Report</title>
            <meta charset="utf-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f0f8ff; padding: 20px; border-radius: 5px; }}
                .model-section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                .success {{ background-color: #f0fff0; }}
                .error {{ background-color: #fff0f0; }}
                table {{ border-collapse: collapse; width: 100%; margin: 10px 0; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
                .metric {{ font-weight: bold; }}
                .warning {{ background-color: #fff3cd; padding: 10px; border-radius: 5px; margin: 10px 0; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>üáªüá≥ Vietnamese Embedding Models Evaluation Report</h1>
                <p><strong>Generated:</strong> {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
                <p><strong>Total Models Tested:</strong> {len(results)}</p>
                <p><strong>Successful Evaluations:</strong> {len(successful_results)}</p>
            </div>
            
            <div class="warning">
                <strong>‚ö†Ô∏è Note:</strong> Using simplified Vietnamese NLP tools (regex-based) instead of underthesea for Windows compatibility.
                For more accurate results, consider using underthesea via conda-forge.
            </div>
            
            <h2>üìä Models Comparison</h2>
            {df.to_html(index=False, escape=False, classes='comparison-table')}
        </body>
        </html>
        """
        
        # Ghi file
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"ƒê√£ t·∫°o b√°o c√°o t·∫°i: {report_file}")
        return report_file
    
    def run_evaluation(self, document_path: str) -> str:
        """
        Ch·∫°y to√†n b·ªô qu√° tr√¨nh ƒë√°nh gi√°
        """
        logger.info("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh ƒë√°nh gi√° embedding models...")
        
        try:
            # 1. Load document
            document = self.load_document_from_md(document_path)
            
            # 2. Chunk text
            chunks = self.chunk_text(document)
            
            if len(chunks) < 2:
                logger.error("C·∫ßn √≠t nh·∫•t 2 chunks ƒë·ªÉ ƒë√°nh gi√°!")
                return None
            
            # 3. Evaluate all models
            results = self.evaluate_all_models(chunks)
            
            # 4. Generate report
            report_file = self.generate_report(results)
            
            logger.info("‚úÖ Ho√†n th√†nh ƒë√°nh gi√°!")
            return report_file
            
        except Exception as e:
            logger.error(f"L·ªói trong qu√° tr√¨nh ƒë√°nh gi√°: {e}")
            raise

# H√†m main ƒë·ªÉ ch·∫°y
def main():
    # Kh·ªüi t·∫°o evaluator
    evaluator = VietnameseEmbeddingEvaluator("config.json")
    
    # Ch·∫°y ƒë√°nh gi√° v·ªõi document
    document_path = "vietnamese_document.md"
    
    if not os.path.exists(document_path):
        logger.error(f"Kh√¥ng t√¨m th·∫•y file document: {document_path}")
        logger.info("Vui l√≤ng t·∫°o file vietnamese_document.md v·ªõi n·ªôi dung ti·∫øng Vi·ªát ƒë·ªÉ test.")
        return
    
    try:
        report_file = evaluator.run_evaluation(document_path)
        if report_file:
            logger.info(f"üìÑ B√°o c√°o ƒë√£ ƒë∆∞·ª£c t·∫°o t·∫°i: {report_file}")
            logger.info("M·ªü file HTML trong tr√¨nh duy·ªát ƒë·ªÉ xem k·∫øt qu·∫£ chi ti·∫øt!")
    except Exception as e:
        logger.error(f"L·ªói: {e}")

if __name__ == "__main__":
    main()