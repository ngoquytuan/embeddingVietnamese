import os
import json
import pandas as pd
from datetime import datetime
from sentence_transformers import SentenceTransformer, util
import numpy as np
from pyvi import ViTokenizer
import underthesea
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import time

class VietnameseEmbeddingTester:
    def __init__(self, output_dir="embedding_test_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.results = {}
        
    def load_document(self, file_path):
        """Load vÄƒn báº£n tá»« file .md hoáº·c .txt"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            print(f"âœ… ÄÃ£ load thÃ nh cÃ´ng file: {file_path}")
            print(f"ğŸ“„ Äá»™ dÃ i vÄƒn báº£n: {len(content)} kÃ½ tá»±")
            return content
        except Exception as e:
            print(f"âŒ Lá»—i khi load file {file_path}: {e}")
            return None
    
    def chunk_text(self, text, chunk_size=200, overlap=50):
        """Chia tÃ i liá»‡u thÃ nh chunks vá»›i overlap Ä‘á»ƒ giá»¯ ngá»¯ nghÄ©a"""
        sentences = underthesea.sent_tokenize(text)
        chunks = []
        current_chunk = ""
        current_word_count = 0
        
        for sentence in sentences:
            words = ViTokenizer.tokenize(sentence).split()
            
            if current_word_count + len(words) <= chunk_size:
                current_chunk += " " + sentence
                current_word_count += len(words)
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                # Táº¡o overlap báº±ng cÃ¡ch giá»¯ láº¡i má»™t pháº§n chunk cÅ©
                overlap_words = current_chunk.split()[-overlap:] if overlap > 0 else []
                current_chunk = " ".join(overlap_words) + " " + sentence
                current_word_count = len(overlap_words) + len(words)
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def create_test_queries(self, chunks):
        """Táº¡o test queries tá»« chunks Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng"""
        # Láº¥y má»™t sá»‘ chunks ngáº«u nhiÃªn lÃ m queries
        num_queries = min(5, len(chunks))
        query_indices = np.random.choice(len(chunks), num_queries, replace=False)
        
        queries = []
        for idx in query_indices:
            # Láº¥y cÃ¢u Ä‘áº§u tiÃªn cá»§a chunk lÃ m query
            chunk_sentences = underthesea.sent_tokenize(chunks[idx])
            if chunk_sentences:
                queries.append({
                    'query': chunk_sentences[0],
                    'relevant_chunk_idx': idx,
                    'relevant_chunk': chunks[idx]
                })
        
        return queries
    
    def evaluate_embedding_model(self, model_name, chunks, queries):
        """ÄÃ¡nh giÃ¡ model embedding vá»›i nhiá»u metrics"""
        try:
            print(f"ğŸ”„ Äang test model: {model_name}")
            start_time = time.time()
            
            # Load model
            model = SentenceTransformer(model_name)
            
            # Encode chunks
            chunk_embeddings = model.encode(chunks, convert_to_tensor=True, show_progress_bar=True)
            
            results = {
                'model_name': model_name,
                'num_chunks': len(chunks),
                'encoding_time': time.time() - start_time,
                'queries_results': []
            }
            
            # Test vá»›i tá»«ng query
            for query_data in queries:
                query_embedding = model.encode([query_data['query']], convert_to_tensor=True)
                similarities = util.cos_sim(query_embedding, chunk_embeddings).cpu().numpy().flatten()
                
                # TÃ¬m top-k similar chunks
                top_k = min(5, len(chunks))
                top_indices = np.argsort(similarities)[::-1][:top_k]
                
                query_result = {
                    'query': query_data['query'],
                    'relevant_chunk_idx': query_data['relevant_chunk_idx'],
                    'top_similarities': similarities[top_indices].tolist(),
                    'top_indices': top_indices.tolist(),
                    'relevant_chunk_rank': None,
                    'relevant_chunk_score': similarities[query_data['relevant_chunk_idx']]
                }
                
                # TÃ¬m rank cá»§a chunk liÃªn quan
                if query_data['relevant_chunk_idx'] in top_indices:
                    query_result['relevant_chunk_rank'] = list(top_indices).index(query_data['relevant_chunk_idx']) + 1
                
                results['queries_results'].append(query_result)
            
            # TÃ­nh metrics tá»•ng há»£p
            avg_relevant_score = np.mean([q['relevant_chunk_score'] for q in results['queries_results']])
            avg_top1_score = np.mean([q['top_similarities'][0] for q in results['queries_results']])
            
            # MRR (Mean Reciprocal Rank)
            reciprocal_ranks = []
            for q in results['queries_results']:
                if q['relevant_chunk_rank']:
                    reciprocal_ranks.append(1.0 / q['relevant_chunk_rank'])
                else:
                    reciprocal_ranks.append(0.0)
            mrr = np.mean(reciprocal_ranks)
            
            results['metrics'] = {
                'avg_relevant_score': float(avg_relevant_score),
                'avg_top1_score': float(avg_top1_score),
                'mrr': float(mrr),
                'encoding_time': results['encoding_time']
            }
            
            print(f"âœ… HoÃ n thÃ nh test {model_name}")
            print(f"ğŸ“Š Avg relevant score: {avg_relevant_score:.4f}")
            print(f"ğŸ“Š MRR: {mrr:.4f}")
            print(f"â±ï¸ Encoding time: {results['encoding_time']:.2f}s")
            
            return results
            
        except Exception as e:
            print(f"âŒ Lá»—i vá»›i model {model_name}: {e}")
            return None
    
    def run_comparison(self, document_path, models, chunk_size=200, overlap=50):
        """Cháº¡y so sÃ¡nh toÃ n diá»‡n cÃ¡c models"""
        print("ğŸš€ Báº¯t Ä‘áº§u so sÃ¡nh embedding models cho tiáº¿ng Viá»‡t")
        print("="*60)
        
        # Load document
        document = self.load_document(document_path)
        if not document:
            return None
        
        # Chunk document
        chunks = self.chunk_text(document, chunk_size=chunk_size, overlap=overlap)
        print(f"ğŸ“ ÄÃ£ chia thÃ nh {len(chunks)} chunks")
        
        # Táº¡o test queries
        queries = self.create_test_queries(chunks)
        print(f"â“ Táº¡o {len(queries)} test queries")
        
        # Test tá»«ng model
        all_results = {}
        for model_name in models:
            result = self.evaluate_embedding_model(model_name, chunks, queries)
            if result:
                all_results[model_name] = result
            print("-" * 40)
        
        self.results = all_results
        return all_results
    
    def generate_report(self, save_detailed=True):
        """Táº¡o bÃ¡o cÃ¡o so sÃ¡nh chi tiáº¿t"""
        if not self.results:
            print("âŒ ChÆ°a cÃ³ káº¿t quáº£ Ä‘á»ƒ táº¡o bÃ¡o cÃ¡o")
            return
        
        print("ğŸ“ˆ Äang táº¡o bÃ¡o cÃ¡o...")
        
        # Táº¡o DataFrame cho so sÃ¡nh
        comparison_data = []
        for model_name, result in self.results.items():
            comparison_data.append({
                'Model': model_name.split('/')[-1],  # Chá»‰ láº¥y tÃªn model
                'Full_Model_Name': model_name,
                'Avg_Relevant_Score': result['metrics']['avg_relevant_score'],
                'Avg_Top1_Score': result['metrics']['avg_top1_score'],
                'MRR': result['metrics']['mrr'],
                'Encoding_Time': result['metrics']['encoding_time'],
                'Num_Chunks': result['num_chunks']
            })
        
        df = pd.DataFrame(comparison_data)
        
        # Sáº¯p xáº¿p theo MRR (metric quan trá»ng nháº¥t)
        df = df.sort_values('MRR', ascending=False)
        
        # In bÃ¡o cÃ¡o console
        print("\n" + "="*80)
        print("ğŸ“Š BÃO CÃO SO SÃNH EMBEDDING MODELS CHO TIáº¾NG VIá»†T")
        print("="*80)
        
        print(f"\nğŸ† RANKING (theo MRR):")
        for idx, row in df.iterrows():
            print(f"{df.index.get_loc(idx) + 1}. {row['Model']}")
            print(f"   ğŸ“ˆ MRR: {row['MRR']:.4f}")
            print(f"   ğŸ¯ Avg Relevant Score: {row['Avg_Relevant_Score']:.4f}")
            print(f"   âš¡ Encoding Time: {row['Encoding_Time']:.2f}s")
            print()
        
        # LÆ°u bÃ¡o cÃ¡o
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # CSV summary
        csv_path = self.output_dir / f"embedding_comparison_{timestamp}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ÄÃ£ lÆ°u bÃ¡o cÃ¡o CSV: {csv_path}")
        
        # JSON chi tiáº¿t
        if save_detailed:
            json_path = self.output_dir / f"detailed_results_{timestamp}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ chi tiáº¿t: {json_path}")
        
        # Táº¡o biá»ƒu Ä‘á»“ so sÃ¡nh
        self._create_comparison_charts(df, timestamp)
        
        return df
    
    def _create_comparison_charts(self, df, timestamp):
        """Táº¡o biá»ƒu Ä‘á»“ so sÃ¡nh"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('So sÃ¡nh Embedding Models cho Tiáº¿ng Viá»‡t', fontsize=16)
            
            # MRR comparison
            axes[0,0].bar(df['Model'], df['MRR'], color='skyblue')
            axes[0,0].set_title('Mean Reciprocal Rank (MRR)')
            axes[0,0].tick_params(axis='x', rotation=45)
            
            # Relevant Score comparison
            axes[0,1].bar(df['Model'], df['Avg_Relevant_Score'], color='lightgreen')
            axes[0,1].set_title('Average Relevant Score')
            axes[0,1].tick_params(axis='x', rotation=45)
            
            # Encoding Time comparison
            axes[1,0].bar(df['Model'], df['Encoding_Time'], color='salmon')
            axes[1,0].set_title('Encoding Time (seconds)')
            axes[1,0].tick_params(axis='x', rotation=45)
            
            # Combined scatter plot
            scatter = axes[1,1].scatter(df['MRR'], df['Avg_Relevant_Score'], 
                                      s=df['Encoding_Time']*10, 
                                      c=range(len(df)), cmap='viridis', alpha=0.7)
            axes[1,1].set_xlabel('MRR')
            axes[1,1].set_ylabel('Avg Relevant Score')
            axes[1,1].set_title('Performance vs Quality (bubble size = encoding time)')
            
            # ThÃªm labels cho scatter plot
            for idx, row in df.iterrows():
                axes[1,1].annotate(row['Model'], 
                                 (row['MRR'], row['Avg_Relevant_Score']),
                                 xytext=(5, 5), textcoords='offset points', fontsize=8)
            
            plt.tight_layout()
            chart_path = self.output_dir / f"comparison_charts_{timestamp}.png"
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ÄÃ£ lÆ°u biá»ƒu Ä‘á»“: {chart_path}")
            plt.show()
            
        except Exception as e:
            print(f"âš ï¸ KhÃ´ng thá»ƒ táº¡o biá»ƒu Ä‘á»“: {e}")

# Sá»­ dá»¥ng chÆ°Æ¡ng trÃ¬nh
if __name__ == "__main__":
    # Khá»Ÿi táº¡o tester
    tester = VietnameseEmbeddingTester()
    
    # Danh sÃ¡ch models Ä‘á»ƒ test
    models_to_test = [
        "mixedbread-ai/mxbai-embed-large-v1",
        "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
        "intfloat/multilingual-e5-large",
        "BAAI/bge-m3",
        # ThÃªm models Viá»‡t Nam náº¿u cÃ³
        # "vinai/phobert-base",  # Cáº§n wrap thÃ nh sentence transformer
    ]
    
    # Cháº¡y test (thay Ä‘á»•i Ä‘Æ°á»ng dáº«n file cá»§a báº¡n)
    document_path = "your_vietnamese_document.md"  # Thay báº±ng file cá»§a báº¡n
    
    # Náº¿u chÆ°a cÃ³ file, táº¡o file máº«u
    if not os.path.exists(document_path):
        sample_doc = """
# VÄƒn hÃ³a Viá»‡t Nam

Viá»‡t Nam lÃ  má»™t quá»‘c gia cÃ³ bá» dÃ y lá»‹ch sá»­ vÃ  vÄƒn hÃ³a phong phÃº. VÄƒn hÃ³a Viá»‡t Nam Ä‘Æ°á»£c hÃ¬nh thÃ nh qua hÃ ng nghÃ¬n nÄƒm lá»‹ch sá»­ vá»›i nhá»¯ng áº£nh hÆ°á»Ÿng tá»« Trung Quá»‘c, áº¤n Äá»™ vÃ  sau nÃ y lÃ  phÆ°Æ¡ng TÃ¢y.

## Kiáº¿n trÃºc truyá»n thá»‘ng

Kiáº¿n trÃºc Viá»‡t Nam mang Ä‘áº­m dáº¥u áº¥n cá»§a vÄƒn hÃ³a phÆ°Æ¡ng ÄÃ´ng vá»›i nhá»¯ng ngÃ´i chÃ¹a, Ä‘á»n thá» cá»• kÃ­nh. VÄƒn Miáº¿u Quá»‘c Tá»­ GiÃ¡m lÃ  biá»ƒu tÆ°á»£ng cá»§a kiáº¿n trÃºc truyá»n thá»‘ng Viá»‡t Nam.

## áº¨m thá»±c

áº¨m thá»±c Viá»‡t Nam ná»•i tiáº¿ng tháº¿ giá»›i vá»›i nhá»¯ng mÃ³n Äƒn Ä‘áº·c trÆ°ng nhÆ° phá»Ÿ, bÃ¡nh mÃ¬, bÃºn cháº£. Má»—i vÃ¹ng miá»n cÃ³ nhá»¯ng Ä‘áº·c sáº£n riÃªng biá»‡t.

## Kinh táº¿ hiá»‡n Ä‘áº¡i

Viá»‡t Nam Ä‘ang phÃ¡t triá»ƒn máº¡nh máº½ vá» kinh táº¿ vá»›i nhiá»u ngÃ nh cÃ´ng nghiá»‡p. CÃ´ng nghá»‡ thÃ´ng tin lÃ  má»™t trong nhá»¯ng lÄ©nh vá»±c phÃ¡t triá»ƒn nhanh nháº¥t.

## Du lá»‹ch

Viá»‡t Nam cÃ³ nhiá»u Ä‘iá»ƒm du lá»‹ch háº¥p dáº«n tá»« Háº¡ Long Bay Ä‘áº¿n Há»™i An, thu hÃºt hÃ ng triá»‡u lÆ°á»£t khÃ¡ch quá»‘c táº¿ má»—i nÄƒm.
        """
        
        with open(document_path, 'w', encoding='utf-8') as f:
            f.write(sample_doc)
        print(f"ğŸ“ ÄÃ£ táº¡o file máº«u: {document_path}")
    
    # Cháº¡y so sÃ¡nh
    results = tester.run_comparison(
        document_path=document_path,
        models=models_to_test,
        chunk_size=150,  # Äiá»u chá»‰nh theo nhu cáº§u
        overlap=30
    )
    
    if results:
        # Táº¡o bÃ¡o cÃ¡o
        comparison_df = tester.generate_report(save_detailed=True)
        
        print("\nğŸ‰ HoÃ n thÃ nh! Kiá»ƒm tra thÆ° má»¥c 'embedding_test_results' Ä‘á»ƒ xem káº¿t quáº£ chi tiáº¿t.")